{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<h1><center> Data Cleaning </center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from autocorrect import spell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Renaming (Inplace) the coloumns as per our flexibilty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Person</th>\n",
       "      <th>VideoId</th>\n",
       "      <th>VideoTitle</th>\n",
       "      <th>Description</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Dislikes</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Emily Ann Shaheen</td>\n",
       "      <td>2mg3sFuiwRw</td>\n",
       "      <td>HOW TO BE AN ARAB GIRL!</td>\n",
       "      <td>Heyyy guys! Thank you so much for watching! Th...</td>\n",
       "      <td>80408</td>\n",
       "      <td>1971</td>\n",
       "      <td>147</td>\n",
       "      <td>640</td>\n",
       "      <td>Im Arab from iraq but my golden name is in eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Emily Ann Shaheen</td>\n",
       "      <td>Wlub9KOJBt4</td>\n",
       "      <td>ARAB GIRL STEREOTYPES!</td>\n",
       "      <td>Thanks for watching babes! xx, Emily Ann Shahe...</td>\n",
       "      <td>27925</td>\n",
       "      <td>608</td>\n",
       "      <td>46</td>\n",
       "      <td>240</td>\n",
       "      <td>Being an Arab is great and proud also true we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nowthisisliving</td>\n",
       "      <td>D2iCOMoOkyI</td>\n",
       "      <td>LESBIAN INTERVIEWS EX BOYFRIEND</td>\n",
       "      <td>please be kind in the comments, this boy is th...</td>\n",
       "      <td>2098024</td>\n",
       "      <td>51987</td>\n",
       "      <td>1644</td>\n",
       "      <td>2718</td>\n",
       "      <td>My ex broke up with me because she wanted to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nowthisisliving</td>\n",
       "      <td>_Uxw2X0hNGg</td>\n",
       "      <td>why we broke up</td>\n",
       "      <td>I know this is a tough video for everyone. We ...</td>\n",
       "      <td>3081184</td>\n",
       "      <td>74616</td>\n",
       "      <td>1766</td>\n",
       "      <td>9406</td>\n",
       "      <td>still high key want them to get back together ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Madison Beer</td>\n",
       "      <td>-9BfaW69LSk</td>\n",
       "      <td>Madison Beer- Catch Me Cover</td>\n",
       "      <td>HEY YOUTUBE!!!!!!!!! LONG TIME NO VIDEO! so so...</td>\n",
       "      <td>920561</td>\n",
       "      <td>14418</td>\n",
       "      <td>2408</td>\n",
       "      <td>2747</td>\n",
       "      <td>Be strong That's my fav song of demi lovato I'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Person      VideoId                       VideoTitle  \\\n",
       "0  Emily Ann Shaheen  2mg3sFuiwRw          HOW TO BE AN ARAB GIRL!   \n",
       "1  Emily Ann Shaheen  Wlub9KOJBt4           ARAB GIRL STEREOTYPES!   \n",
       "2    nowthisisliving  D2iCOMoOkyI  LESBIAN INTERVIEWS EX BOYFRIEND   \n",
       "3    nowthisisliving  _Uxw2X0hNGg                  why we broke up   \n",
       "4       Madison Beer  -9BfaW69LSk     Madison Beer- Catch Me Cover   \n",
       "\n",
       "                                         Description ViewCount  Likes  \\\n",
       "0  Heyyy guys! Thank you so much for watching! Th...     80408   1971   \n",
       "1  Thanks for watching babes! xx, Emily Ann Shahe...     27925    608   \n",
       "2  please be kind in the comments, this boy is th...   2098024  51987   \n",
       "3  I know this is a tough video for everyone. We ...   3081184  74616   \n",
       "4  HEY YOUTUBE!!!!!!!!! LONG TIME NO VIDEO! so so...    920561  14418   \n",
       "\n",
       "  Dislikes CommentCount                                           Comments  \n",
       "0      147          640  Im Arab from iraq but my golden name is in eng...  \n",
       "1       46          240  Being an Arab is great and proud also true we ...  \n",
       "2     1644         2718  My ex broke up with me because she wanted to b...  \n",
       "3     1766         9406  still high key want them to get back together ...  \n",
       "4     2408         2747  Be strong That's my fav song of demi lovato I'...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('Data/videoInfo.pkl')\n",
    "df.rename(columns=\n",
    "          {\"View Count\": \"ViewCount\",\n",
    "           \"Comment Count\": \"CommentCount\",\n",
    "            \"Uploader\":\"Person\"\n",
    "          }, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting only person and comments he or she received from each of the videos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. nine coloums or features for 16 videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of                Person      VideoId  \\\n",
       "0   Emily Ann Shaheen  2mg3sFuiwRw   \n",
       "1   Emily Ann Shaheen  Wlub9KOJBt4   \n",
       "2     nowthisisliving  D2iCOMoOkyI   \n",
       "3     nowthisisliving  _Uxw2X0hNGg   \n",
       "4        Madison Beer  -9BfaW69LSk   \n",
       "5        Madison Beer  9VksH5TlLXA   \n",
       "6             rebecca  LyFiWkmkexI   \n",
       "7             rebecca  KVPdFHy-L7Q   \n",
       "8     Madeline & Eric  9tulMlAMnfs   \n",
       "9     Madeline & Eric  PClZUEc2zOs   \n",
       "10       Charlie Ayee  B7QLMsNiQyg   \n",
       "11  AlienRadioStation  t8K44P55wJA   \n",
       "12      Danielle Cohn  ePyTreCDiOg   \n",
       "13      Danielle Cohn  l5P2LTS-MmQ   \n",
       "14          RoughChop  __IjcLVBBYc   \n",
       "15          RoughChop  owVFNY1PBsA   \n",
       "\n",
       "                                           VideoTitle  \\\n",
       "0                             HOW TO BE AN ARAB GIRL!   \n",
       "1                              ARAB GIRL STEREOTYPES!   \n",
       "2                     LESBIAN INTERVIEWS EX BOYFRIEND   \n",
       "3                                     why we broke up   \n",
       "4                        Madison Beer- Catch Me Cover   \n",
       "5                              Madison Beer - Bulgari   \n",
       "6                          Sweetheart - Rebecca Black   \n",
       "7                           Rebecca Black - Satellite   \n",
       "8   DAY IN THE LIFE OF A PREGNANT TEEN IN HIGH SCHOOL   \n",
       "9                          TEEN PARENTS NIGHT ROUTINE   \n",
       "10  YOUTUBER fatboygetdown fat boy get down- DANCE...   \n",
       "11                                  My 600 pound meme   \n",
       "12                                    OUR FIRST TIME!   \n",
       "13  CHEATING with Boyfriends Best friend PRANK *go...   \n",
       "14  Bully Richard Gale Interview (Bully of Casey H...   \n",
       "15   Bully Richard Gale - NEW WITNESS - The Cameraman   \n",
       "\n",
       "                                          Description ViewCount  Likes  \\\n",
       "0   Heyyy guys! Thank you so much for watching! Th...     80408   1971   \n",
       "1   Thanks for watching babes! xx, Emily Ann Shahe...     27925    608   \n",
       "2   please be kind in the comments, this boy is th...   2098024  51987   \n",
       "3   I know this is a tough video for everyone. We ...   3081184  74616   \n",
       "4   HEY YOUTUBE!!!!!!!!! LONG TIME NO VIDEO! so so...    920561  14418   \n",
       "5   dare to be yourself in every situation and sha...     69216   6034   \n",
       "6   Listen to \"Sweetheart\" Now\\nhttps://spoti.fi/2...    156981  10412   \n",
       "7   Stream \"Satellite\" from Rebecca Black's EP \"RE...   1049192  39807   \n",
       "8   Hey Guys !!\\nWe hope you guys enjoyed our day ...   1735269  22605   \n",
       "9   Hey guys !! \\nWELCOME back to our channel, we ...    504102   8845   \n",
       "10  youtuber fatboygetdown dances to my hump... CH...    435783    986   \n",
       "11                        AHHHHHHHHHHHHHHHHHHHHHHHHHH   1512460  24902   \n",
       "12  WHAT WAS YOUR CRAZIEST FIRST TIME!!\\n \\nFOLLOW...   4406667  51579   \n",
       "13  I Cant Believe Mikey reacted this way!!\\n\\nTha...   1158164  23724   \n",
       "14  Watch Casey Heynes Latest Interview http://www...  11336365  17817   \n",
       "15  Watch Casey Heynes Latest Interview http://www...    693569   1028   \n",
       "\n",
       "   Dislikes CommentCount                                           Comments  \n",
       "0       147          640  Im Arab from iraq but my golden name is in eng...  \n",
       "1        46          240  Being an Arab is great and proud also true we ...  \n",
       "2      1644         2718  My ex broke up with me because she wanted to b...  \n",
       "3      1766         9406  still high key want them to get back together ...  \n",
       "4      2408         2747  Be strong That's my fav song of demi lovato I'...  \n",
       "5        82          295  angel energy Her lips magically grew so much 2...  \n",
       "6       362         1178  My old girlfriend also had a book titled Knot ...  \n",
       "7      2665         5998  It's Friday, it's Friday.....    Getting down ...  \n",
       "8      1034          922  Your grocery carts are sooo small. Ours look g...  \n",
       "9       344          289  HEY GUYS !!\\nJust wanting to clarify something...  \n",
       "10      540         1067  Steven is a Sagittarius, we be great is my guy...  \n",
       "11     4101         4460  Thats 1000000 memes 1000 like I taught this wa...  \n",
       "12    40627        18251  Disgusting So u 13 and you had a first time Th...  \n",
       "13    10223         9647  This is so fake Jeon Kookie bruh how ig mikey ...  \n",
       "14   101731        80989  Hes a bitch Richard your a n idiot mate He bul...  \n",
       "15     4947         3434  that's an ugly mothafocka Liar I knew it hey n...  >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we have comments as per in individual video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Person</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Emily Ann Shaheen</td>\n",
       "      <td>Im Arab from iraq but my golden name is in eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Emily Ann Shaheen</td>\n",
       "      <td>Being an Arab is great and proud also true we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nowthisisliving</td>\n",
       "      <td>My ex broke up with me because she wanted to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nowthisisliving</td>\n",
       "      <td>still high key want them to get back together ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Madison Beer</td>\n",
       "      <td>Be strong That's my fav song of demi lovato I'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Person                                           Comments\n",
       "0  Emily Ann Shaheen  Im Arab from iraq but my golden name is in eng...\n",
       "1  Emily Ann Shaheen  Being an Arab is great and proud also true we ...\n",
       "2    nowthisisliving  My ex broke up with me because she wanted to b...\n",
       "3    nowthisisliving  still high key want them to get back together ...\n",
       "4       Madison Beer  Be strong That's my fav song of demi lovato I'..."
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_comments = df[['Person','Comments']]\n",
    "person_comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some videos I chose are uploaded by uploader, but subjects are different in the content. Regardless, I would like to analyze comments in the videos. \n",
    "Hence, I am replacing the person name by subject of the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Changing the name of some subjects as uploader are different in this case but the videos are of our concern topic and of our concering celebrites also"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpint ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Person</th>\n",
       "      <td>Emily Ann Shaheen</td>\n",
       "      <td>Emily Ann Shaheen</td>\n",
       "      <td>Shannon Beveridge</td>\n",
       "      <td>Shannon Beveridge</td>\n",
       "      <td>Madison Beer</td>\n",
       "      <td>Madison Beer</td>\n",
       "      <td>Rebecca Black</td>\n",
       "      <td>Rebecca Black</td>\n",
       "      <td>Madeline &amp; Eric</td>\n",
       "      <td>Madeline &amp; Eric</td>\n",
       "      <td>Steven Assanti</td>\n",
       "      <td>Steven Assanti</td>\n",
       "      <td>Danielle Cohn</td>\n",
       "      <td>Danielle Cohn</td>\n",
       "      <td>Richard Gale</td>\n",
       "      <td>Richard Gale</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0                  1                  2   \\\n",
       "Person  Emily Ann Shaheen  Emily Ann Shaheen  Shannon Beveridge   \n",
       "\n",
       "                       3             4             5              6   \\\n",
       "Person  Shannon Beveridge  Madison Beer  Madison Beer  Rebecca Black   \n",
       "\n",
       "                   7                8                9               10  \\\n",
       "Person  Rebecca Black  Madeline & Eric  Madeline & Eric  Steven Assanti   \n",
       "\n",
       "                    11             12             13            14  \\\n",
       "Person  Steven Assanti  Danielle Cohn  Danielle Cohn  Richard Gale   \n",
       "\n",
       "                  15  \n",
       "Person  Richard Gale  "
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Replacing Person with actual subject in the video\n",
    "person_comments = person_comments.replace({'Person' : {'Charlie Ayee': 'Steven Assanti', \n",
    "                               'AlienRadioStation' : 'Steven Assanti',\n",
    "                               'rebecca' : 'Rebecca Black',\n",
    "                               'nowthisisliving':'Shannon Beveridge',\n",
    "                               'RoughChop':'Richard Gale'}})\n",
    "person_comments['Person'].to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining subjects in one row**:\n",
    "\n",
    "Each subject has multiple videos and comment pairs. I am interested in finding what kind of comments each uploader gets. In this light I can combine comments from multiple videos into one for a user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combining all videos comment of a particular uploader into one for determing the type of comment each uploader gets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Person</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Danielle Cohn</td>\n",
       "      <td>Disgusting So u 13 and you had a first time Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Emily Ann Shaheen</td>\n",
       "      <td>Im Arab from iraq but my golden name is in eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Madeline &amp; Eric</td>\n",
       "      <td>Your grocery carts are sooo small. Ours look g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Madison Beer</td>\n",
       "      <td>Be strong That's my fav song of demi lovato I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rebecca Black</td>\n",
       "      <td>My old girlfriend also had a book titled Knot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Richard Gale</td>\n",
       "      <td>Hes a bitch Richard your a n idiot mate He bul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shannon Beveridge</td>\n",
       "      <td>My ex broke up with me because she wanted to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Steven Assanti</td>\n",
       "      <td>Steven is a Sagittarius, we be great is my guy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Person                                           Comments\n",
       "0      Danielle Cohn  Disgusting So u 13 and you had a first time Th...\n",
       "1  Emily Ann Shaheen  Im Arab from iraq but my golden name is in eng...\n",
       "2    Madeline & Eric  Your grocery carts are sooo small. Ours look g...\n",
       "3       Madison Beer  Be strong That's my fav song of demi lovato I'...\n",
       "4      Rebecca Black  My old girlfriend also had a book titled Knot ...\n",
       "5       Richard Gale  Hes a bitch Richard your a n idiot mate He bul...\n",
       "6  Shannon Beveridge  My ex broke up with me because she wanted to b...\n",
       "7     Steven Assanti  Steven is a Sagittarius, we be great is my guy..."
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_comments_grouped=person_comments.groupby(['Person'])['Comments'].apply(','.join).reset_index()\n",
    "person_comments_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickling grouped unclean data for later use\n",
    "person_comments_grouped.to_pickle(\"person_comments_grouped.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Im Arab from iraq but my golden name is in english üòÇ You remind me so much of rclbeauty101.. I hope to marry an Arab woman one day. I never noticed how attractive I find them until I became an adult I love it! I sooooo have a necklace with my name in Arabic and I\\'m very proud of it! Thank you for your videos! Much love to you from another Arab-American \"sister\"! I thought I\\'d get some tips to look like an arabian Girl üòÇüòÇüòÇüòÇ I\\'m literally the hairiest fuckin girl in my class(ALL ARABS) and I can\\'t do about it.... except mustacge and brows I‚Äôm not Arab I‚Äôm Somali \\nMy name is hard to say\\nI have a gold necklace \\nIt says my name on it  \\nLike if you got a gold \\nNecklace Wanna be a arab girl? Hate Men! Abdullah Ahmed more like \\nWanna be a brainwashed SJW? hate men Where did you get your necklace?!üíñ Is middle east a new continent \\n Since when the middle east became a new continent its not a continent... Middle East is part of Asia. I\\'m arab(Algeriaaaaaaüá©üáøüá©üáøüá©üáøüá©üáøüá©üáø) and my face is covered with mo'"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at a sample text:\n",
    "person_comments_grouped['Comments'][1][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "\n",
    "**Common data cleaning steps on all text:**\n",
    "\n",
    "* Make text lower case\n",
    "* Remove punctuation\n",
    "* Remove numerical values\n",
    "* Remove common non-sensical text\n",
    "* Tokenize text\n",
    "* Remove stop words\n",
    "* Removing emojis and icons\n",
    "\n",
    "**More data cleaning steps after tokenization:**\n",
    "\n",
    "* Stemming Lemmatization\n",
    "* Deal with typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleaning\n",
    "\n",
    "def textCleaning(text):\n",
    "    text = re.sub('[%s]' % re.escape(punctuation), ' ', text)  #remove punctuation\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text)                       #remove words with numbers\n",
    "    text = re.sub('[‚Äò‚Äô‚Äú‚Äù‚Ä¶]', ' ', text)\n",
    "    text = text.lower()                                        # make lowercase\n",
    "    text = re.sub('\\n', ' ', text)                             # remove new line\n",
    "    return text\n",
    "\n",
    "\n",
    "text_cleaner = lambda x: textCleaning(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint ______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Emojis\n",
    "\n",
    "def removing_emojis(text):\n",
    "    # Emojis pattern\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                    u\"\\U00002702-\\U000027B0\"\n",
    "                    u\"\\U000024C2-\\U0001F251\"\n",
    "                    u\"\\U0001f926-\\U0001f937\"\n",
    "                    u'\\U00010000-\\U0010ffff'\n",
    "                    u\"\\u200d\"\n",
    "                    u\"\\u2640-\\u2642\"\n",
    "                    u\"\\u2600-\\u2B55\"\n",
    "                    u\"\\u23cf\"\n",
    "                    u\"\\u23e9\"\n",
    "                    u\"\\u231a\"\n",
    "                    u\"\\u3030\"\n",
    "                    u\"\\ufe0f\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r' ', text) \n",
    "\n",
    "emojiRemoval = lambda x: removing_emojis(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "lemma = WordNetLemmatizer()\n",
    "def lemmatization(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_text=[]\n",
    "    for w in words:\n",
    "        w= lemma.lemmatize(w)\n",
    "        lemmatized_text.append(w)\n",
    "    return ' '.join(lemmatized_text)\n",
    "\n",
    "lemmatizor = lambda x: lemmatization(x)\n",
    "\n",
    "\n",
    "#Stemming\n",
    "porter = PorterStemmer()\n",
    "def stemming(text):\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_text = []\n",
    "    for w in words:\n",
    "        w = porter.stem(w)\n",
    "        stemmed_text.append(w)\n",
    "    return ' '.join(stemmed_text)\n",
    "\n",
    "stemmor = lambda x: stemming(x)\n",
    "\n",
    "\n",
    "#Spell Checking\n",
    "def spellCheck(text):\n",
    "    autocorrected_text = spell(text)\n",
    "    return autocorrected_text\n",
    "\n",
    "spell_checker = lambda x: spellCheck(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing Data \n",
    "\n",
    "\n",
    "### Corpus \n",
    "\n",
    "Corpus is a collection of text. Here we are creating collection of cleaned texts. \n",
    "\n",
    "Let us apply different text cleaning functions we created above.\n",
    "\n",
    "For now, I am skipping lemmatization, stemming and spell checking. The reason for this is because they tend to correct per change spellings of texts. In this project, we are looking for abuse words too. We don't want such words to be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Skipping the stemming and lemmitization part as here we have abuse words too , and we dont want there spelling to get changed while cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Person</th>\n",
       "      <th>Cleaned_Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Danielle Cohn</td>\n",
       "      <td>disgusting so u   and you had a first time thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Emily Ann Shaheen</td>\n",
       "      <td>im arab from iraq but my golden name is in eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Madeline &amp; Eric</td>\n",
       "      <td>your grocery carts are sooo small  ours look g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Madison Beer</td>\n",
       "      <td>be strong that s my fav song of demi lovato i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rebecca Black</td>\n",
       "      <td>my old girlfriend also had a book titled knot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Richard Gale</td>\n",
       "      <td>hes a bitch richard your a n idiot mate he bul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shannon Beveridge</td>\n",
       "      <td>my ex broke up with me because she wanted to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Steven Assanti</td>\n",
       "      <td>steven is a sagittarius  we be great is my guy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Person                                   Cleaned_Comments\n",
       "0      Danielle Cohn  disgusting so u   and you had a first time thi...\n",
       "1  Emily Ann Shaheen  im arab from iraq but my golden name is in eng...\n",
       "2    Madeline & Eric  your grocery carts are sooo small  ours look g...\n",
       "3       Madison Beer  be strong that s my fav song of demi lovato i ...\n",
       "4      Rebecca Black  my old girlfriend also had a book titled knot ...\n",
       "5       Richard Gale  hes a bitch richard your a n idiot mate he bul...\n",
       "6  Shannon Beveridge  my ex broke up with me because she wanted to b...\n",
       "7     Steven Assanti  steven is a sagittarius  we be great is my guy..."
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_clean= person_comments_grouped['Comments'].apply(text_cleaner).apply(emojiRemoval)\n",
    "               #.apply(lemmatizor).apply(stemmor).apply(spell_checker)\n",
    "\n",
    "person_comments_cleaned= person_comments_grouped.drop(['Comments'], axis=1)\n",
    "person_comments_cleaned.insert(1, 'Cleaned_Comments', comment_clean)\n",
    "\n",
    "person_comments_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. We can see that comments got cleaned but the spelling is not changed as we have not done stemming and lemmitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'im arab from iraq but my golden name is in english   you remind me so much of     i hope to marry an arab woman one day  i never noticed how attractive i find them until i became an adult i love it  i sooooo have a necklace with my name in arabic and i m very proud of it  thank you for your videos  much love to you from another arab american  sister   i thought i d get some tips to look like an arabian girl   i m literally the hairiest fuckin girl in my class all arabs  and i can t do about it     except mustacge and brows i m not arab i m somali  my name is hard to say i have a gold necklace  it says my name on it   like if you got a gold  necklace wanna be a arab girl  hate men  abdullah ahmed more like  wanna be a brainwashed sjw  hate men where did you get your necklace    is middle east a new continent   since when the middle east became a new continent its not a continent    middle east is part of asia  i m arab algeriaaaaaa   and my face is covered with moles and i literally sho'"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at a sample text:\n",
    "person_comments_cleaned['Cleaned_Comments'][1][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickling cleaned data for later use\n",
    "person_comments_cleaned.to_pickle(\"person_comments_cleaned.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Zipping the person and cleaned comments into a dictonary and then combing them all into a single large piece of text as we have cleaned the text above so theri might be white space in between them then again converting it back to dataframe for the further analysic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Danielle Cohn</th>\n",
       "      <td>disgusting so u   and you had a first time thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emily Ann Shaheen</th>\n",
       "      <td>im arab from iraq but my golden name is in eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Madeline &amp; Eric</th>\n",
       "      <td>your grocery carts are sooo small  ours look g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Madison Beer</th>\n",
       "      <td>be strong that s my fav song of demi lovato i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rebecca Black</th>\n",
       "      <td>my old girlfriend also had a book titled knot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Richard Gale</th>\n",
       "      <td>hes a bitch richard your a n idiot mate he bul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shannon Beveridge</th>\n",
       "      <td>my ex broke up with me because she wanted to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Steven Assanti</th>\n",
       "      <td>steven is a sagittarius  we be great is my guy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            comments\n",
       "Danielle Cohn      disgusting so u   and you had a first time thi...\n",
       "Emily Ann Shaheen  im arab from iraq but my golden name is in eng...\n",
       "Madeline & Eric    your grocery carts are sooo small  ours look g...\n",
       "Madison Beer       be strong that s my fav song of demi lovato i ...\n",
       "Rebecca Black      my old girlfriend also had a book titled knot ...\n",
       "Richard Gale       hes a bitch richard your a n idiot mate he bul...\n",
       "Shannon Beveridge  my ex broke up with me because she wanted to b...\n",
       "Steven Assanti     steven is a sagittarius  we be great is my guy..."
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Converting to the dictonary\n",
    "personComments_dict = dict(zip(person_comments_cleaned.Person, person_comments_cleaned.Cleaned_Comments))\n",
    "\n",
    "\n",
    "# This function converts strings into single large peice of text removing white spaces\n",
    "def combine_text(list_of_text):\n",
    "    combined_text = ''.join(list_of_text)\n",
    "    return combined_text\n",
    "\n",
    "\n",
    "\n",
    "person_comments_cleaned_kvp = {key: [combine_text(value)] for (key, value) in personComments_dict.items()}\n",
    "\n",
    "# Again converting back to the data frame\n",
    "personComments_corpus = pd.DataFrame.from_dict(person_comments_cleaned_kvp).transpose()\n",
    "\n",
    "personComments_corpus.columns = ['comments']\n",
    "# Sort on the basis of index values\n",
    "personComments_corpus = personComments_corpus.sort_index()\n",
    "personComments_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.Pickling the above file into local system for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickling corpus for later use\n",
    "personComments_corpus.to_pickle('personComments_corpus.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Major goal of this part\n",
    "## 8. Finally making a  Document Term Matrix (DTM)\n",
    "\n",
    "Document Term Matrix is word counts in matrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aaaaaaaaaaaaa</th>\n",
       "      <th>aaaaaaakward</th>\n",
       "      <th>aaw</th>\n",
       "      <th>aawww</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abbara</th>\n",
       "      <th>...</th>\n",
       "      <th>Ÿäÿß</th>\n",
       "      <th>Ÿäÿßÿ≥ŸÖŸäŸÜ</th>\n",
       "      <th>Ÿäÿ™ÿØŸÑŸâ</th>\n",
       "      <th>ŸäÿØŸäŸá</th>\n",
       "      <th>Ÿäÿ≥ÿ™Ÿàÿπÿ®Ÿáÿß</th>\n",
       "      <th>Ÿäÿ¥ÿ®Ÿá</th>\n",
       "      <th>Ÿäÿπÿßÿ±ÿ∂</th>\n",
       "      <th>·¥õ ú…™s</th>\n",
       "      <th>·¥õ ú·¥ú·¥ç ô…¥·¥Ä…™ ü</th>\n",
       "      <th>·¥°·¥õ“ì</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Danielle Cohn</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emily Ann Shaheen</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Madeline &amp; Eric</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Madison Beer</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rebecca Black</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Richard Gale</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shannon Beveridge</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Steven Assanti</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows √ó 7143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   aaa  aaaa  aaaaaaaaaaaaa  aaaaaaakward  aaw  aawww  ab  \\\n",
       "Danielle Cohn        0     0              0             0    0      0   0   \n",
       "Emily Ann Shaheen    0     0              0             0    0      1   0   \n",
       "Madeline & Eric      1     1              1             0    1      0   0   \n",
       "Madison Beer         0     0              0             0    0      0   0   \n",
       "Rebecca Black        0     0              0             0    0      0   0   \n",
       "Richard Gale         0     0              0             0    0      0   1   \n",
       "Shannon Beveridge    1     0              0             1    0      0   0   \n",
       "Steven Assanti       0     0              0             0    0      0   0   \n",
       "\n",
       "                   abandon  abandoned  abbara  ...  Ÿäÿß  Ÿäÿßÿ≥ŸÖŸäŸÜ  Ÿäÿ™ÿØŸÑŸâ  ŸäÿØŸäŸá  \\\n",
       "Danielle Cohn            0          0       0  ...   0       0      0     0   \n",
       "Emily Ann Shaheen        0          0       2  ...   1       1      1     1   \n",
       "Madeline & Eric          1          0       0  ...   0       0      0     0   \n",
       "Madison Beer             0          0       0  ...   0       0      0     0   \n",
       "Rebecca Black            0          0       0  ...   0       0      0     0   \n",
       "Richard Gale             0          0       0  ...   0       0      0     0   \n",
       "Shannon Beveridge        0          0       0  ...   0       0      0     0   \n",
       "Steven Assanti           0          1       0  ...   0       0      0     0   \n",
       "\n",
       "                   Ÿäÿ≥ÿ™Ÿàÿπÿ®Ÿáÿß  Ÿäÿ¥ÿ®Ÿá  Ÿäÿπÿßÿ±ÿ∂  ·¥õ ú…™s  ·¥õ ú·¥ú·¥ç ô…¥·¥Ä…™ ü  ·¥°·¥õ“ì  \n",
       "Danielle Cohn             0     0      0     1          1    1  \n",
       "Emily Ann Shaheen         1     1      1     0          0    0  \n",
       "Madeline & Eric           0     0      0     0          0    0  \n",
       "Madison Beer              0     0      0     0          0    0  \n",
       "Rebecca Black             0     0      0     0          0    0  \n",
       "Richard Gale              0     0      0     0          0    0  \n",
       "Shannon Beveridge         0     0      0     0          0    0  \n",
       "Steven Assanti            0     0      0     0          0    0  \n",
       "\n",
       "[8 rows x 7143 columns]"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating document-term matrix using CountVectorizer and excluding common English stop words\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(personComments_corpus.comments)\n",
    "personComments_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "personComments_dtm.index = personComments_corpus.index\n",
    "personComments_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "personComments_dtm.to_pickle(\"personComments_dtm.pkl\")\n",
    "pickle.dump(cv, open(\"cv.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
